\documentclass{article} % For LaTeX2e
% We will use NIPS submission format
\usepackage{nips13submit_e,times}
% for hyperlinks
\usepackage{hyperref}
\usepackage{placeins}
\usepackage{url}
% For figures
\usepackage{graphicx} 
% math packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsopn}
\usepackage{ifthen}
\usepackage{natbib}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{booktabs}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

\title{Convolutional Neural Networks using separable filters}
\author{
\fontsize{8}{8}\selectfont{Petrescu Viviana}\\
\fontsize{8}{8}\selectfont{EPFL} \\
\fontsize{8}{8}\selectfont{\texttt{viviana.petrescu@epfl.ch}} \\
}

\nipsfinalcopy 

\begin{document}

\maketitle

\begin{abstract}
Convolutional Neural Networks (CNNs) recently achieved state-of-the-art performance on various computer vision tasks ranging from the large scale ImageNet object recognition challenge to segmentation in bio medical imaging.
This technical report presents initial results on using CNN with separable filters for speeding up their execution.
\end{abstract}

\section{Introduction}
Although proven to be very powerful, CNN are much slower for both training and testing than their counter parts SVM or Random Forests.
In the forward pass, the computational complexity of evaluating one image of size $W\times H$ with $J$ filters of size $d_{1}\times d_{2}$ is $O(WHJd_{1}d_{2})$.
Improving the training time would be very beneficial since it would allow for more configurations to be tried and for larger networks. However, increasing effort has been put also into speeding up only testing time, since training can be done offline.

\section{Speeding up CNN}
The most common approach for speeding up CNNs is to run them on the GPU, making use of the paralelism nature of the algorithm. Alternatively or combined, FFT can be used for the convolutional operations (for  both training and testing)\cite{DBLP:journals/corr/MathieuHL13}.  For testing, a significant speedup can be obtained in hardware by using FGPAs \citep{lecun2010convolutional}. The major drawback of FGPAs is that they are harder to program for people who do not work in the field.

Recently,\citep{Jaderberg14b}  \citep{DBLP:dblp_journals/pami/SironiTRLF15} have showed that separable filters can be used to speed up convolutions with no loss in performance.
In \citep{DBLP:dblp_journals/pami/SironiTRLF15} a non separable filter bank is approximated with a smaller set of separable filters. Their approach is applied to several computer vision problems and MNIST among them. \textcolor{red}{bla} 

In \citep{Jaderberg14b} they prove speedup of CNNs for two different schemes using separable filters, one of which is similar with our approach but uses a different optimization algorithm for obtaining the separable filters. 
  If the $2D$ filters are decomposed into a set of separable $1D$ filters of rank $K$, the complexity per image becomes
 $O(WH(J +d_{1}+d_{2}))$. Thus, we obtain a speedup if $K<< \frac{Jd_{1}d_{2}}{J +d_{1}+d_{2}}$. 

\section{Separable filters}
In our work, we consider the approach of \citep{DBLP:dblp_journals/pami/SironiTRLF15}, where the set of filters $\chi$ for one convolutional layer is approximated as
a set of separable filters, as shown in Fig\ref{fig:decomposition} (from \citep{KoBa09}).
\begin{figure}[h]
  \centering
   \includegraphics[width=\textwidth]{images/decomposable.pdf}
  \caption{Tensor decomposition. A general 3D tensor  $\chi \in \mathbb{R}^{I\times J\times K}$ is approximated by a set of separable filters of rank  R}
  \label{fig:decomposition}
\end{figure}

We can obtain an approximation by minimizing the equation:
\begin{equation*}
\begin{aligned}
& \underset{a,b,c}{\text{minimize}}
& \| \chi - \sum_{r=1}^{R}{a_{r}\circ b_{r}\circ c_{r}} \|_{2}^{2} 
\end{aligned}
\end{equation*}
We optimize the above equation using a Matlab implementation of the Canonical Polyadic decomposition (a generalization of  SVD for tensors) with  non linear conjugate gradient method. The only parameter of the tool is the rank $R$, which is almost never known.
R represents the number of separable filters used to approximate the original filter bank. When the rank used for decomposition is not the theoretical one, the decomposition is not very accurate and the algorithm needs to be run with different initialization parameter until it converges.
What is known is an upper bound on the theoretical rank $R$ for a  general tensor $3D$ tensor $\chi \in \mathbb{R}^{I\times J\times K}$, given by:
 \begin{equation*}
\begin{aligned}
& \text{rank}(\chi) \leq 
& min{\{IJ, JK, KI\}}
\end{aligned}
\end{equation*} 
Besides the theoretical rank, there is also the notion of typical rank, which is any rank
that appears with probability greater than 0, or that it is most common \cite{KoBa09}.
Some of the known typical ranks are shown in Table \ref{table:rank}.
Looking at the first row of Table \ref{table:rank}, we conclude that for a 'very tall' set of filters like the ones present in large CNNs, 
we should expect the rank to be equal with the product of the two kernel dimensions.
 \begin{table}
\centering
\begin{tabular}{@{}ll@{}}\toprule
Tensor size & Typical rank \\ \midrule
$I \times J \times K$ with $JK \leq I$ (very tall) & $JK$\\
$I \times J \times K$ with $JK - J < I < JK$ (tall) & $I$ \\
$I \times J \times K$ with $I = JK - J$ (compact) & $I, I+1$  \\ \bottomrule
\end{tabular}
\caption{Typical rank for selected tensor sizes. A typical rank is any rank which appears with great probability in practice.}
\label{table:rank}
\end{table}

\section{Experiments}
We used a Python implementation of CNN using the Theano library and experimented on two datasets, MNIST and Mitocondria Striatum. 
We varied the rank for every convolutional layer and compared the separable version with the non separable one. We recorded the time, the change in performance and how well the separable filters approximate the $2D$ filters. 
\include{MNIST}
\include{Mitocondria}

\section{Conclusions}
In our experiments, the execution time of CNN models with separable filters was always smaller than that of the original models with non separable filters. The actual fold of the speedup is implementation specific.

The filter bank of the first convolutional layer in a CNN consists in basic building blocks upon which more complex filters are learned in the following layers. Its approximation using separable filters of lower rank  can be very accurate (as seen on MNIST and ImageNet models). However, using an approximation with relatively bad fit for the first layer affects the classification accuracy more than an equivalent low fit for the later layers (at least for the MNIST model).

For very deep networks with small kernel sizes, we need to use a rank without perfect fit (e.g. $\leq 90$ in order to obtain a 2-fold speedup. Even if the fit is not very accurate, we do not expect the accuracy performance to drop so much, especially if we relearn the weights of the last layer. We plan to investigate this further.

As future work, a different approach that exploits separable filters and speeds up the computation is to learn the separable filters directly. This will not only improve the execution time, but we expect the performance of the net to be similar with a net that uses 2D filters since we do not need to sub-estimate the rank or relearn the weights of the network.

\section*{Acknowledgments}
I would like to thank Simon Arosi who helped me a lot during this project giving me constant feedback.

\bibliography{citations}
\bibliographystyle{plain}

\end{document}

